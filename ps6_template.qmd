---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Peter Ganong, Maggie Shi, and Andre Oviedo"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*\_\_\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*\_\_\*\* (2 point)
3. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: true
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import pandas as pd
import altair as alt 
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 

import json
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
sample=pd.read_csv(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\waze_data_sample.csv")
sample.head()
```
Unnamed 0: Index
city: Nominal
confidence: Quantitative
nThumbsUp: Quantitative
street: Nominal
uuid: Nominal
country: Nominal
type: Nominal
subtype: Nominal
RoadType: Nominal
reliability: Quantitative
magvar: Nominal
reportRating: Quantitative

2. 

```{python}
df=pd.read_csv(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\waze_data.csv")
```

```{python}
def count_null(data):
  count_null=[]
  for column in data:
    num_null=len(data[data[column]=='NULL'])
    count_null.append(num_null)
  return count_null
```

```{python}
df_null_counts=count_null(df)
print(df_null_counts)
```
```{python}
def count_na(data):
  counts=[]
  for column in data:
    num_na=len(data[data[column].isna()==True])
    counts.append(num_na)
  return counts
```

```{python}
def not_na(data):
  count_not_na=[]
  for column in data:
    num_not_na=len(data[data[column].isna()==False])
    count_not_na.append(num_not_na)
  return count_not_na
```
```{python}
na_counts=count_na(df)
not_na_counts=not_na(df)
variables=df.columns
counts = pd.DataFrame({'Variable': variables, 'Na_Count': na_counts, 'Not_Na_Count': not_na_counts
})
```

```{python}
counts_melt=pd.melt(counts, id_vars=['Variable'],value_vars=['Na_Count', 'Not_Na_Count'])
counts_melt.columns=['Variable', 'Type', 'Count']
counts_melt.head()
```


```{python}
stacked_bar_na=alt.Chart(counts_melt, title='Number of Missing and Not Missing Observations by Variable').mark_bar().encode(
  alt.X('Variable:N'),
  alt.Y('Count:Q'),
  alt.Color('Type:N')
)
stacked_bar_na
```
nThumbsUp, street, and subtype all have missing values. nThumbsUp has the highest share of missing observations.


3. 

```{python}
print(df['type'].unique())
for i in (df['type'].unique()):
  subset=df[df['type']==i]
  print(f'{i}: {subset['subtype'].unique()
  }')
```
All 4 types have a nan subtype.
Hazard could have sub-subtypes- on road, on shoulder, and weather. 

*Jam
  *Na
  *Stand Still Traffic
  *Heavy Traffic
  *Moderate Traffic
  *Light Traffic
*Accident
  *Na
  *Major
  *Minor
*Road Closed
  *Na
  *Event
  *Construction
  *Hazard
*Hazard
  *Na
  *On Road
    *General
    *Car Stopped
    *Construction
    *Emergency Vehicle
    *Ice
    *Object
    *Pot Hole
    *Traffic Light Fault
    *Lane Closed
    *Road Kill
  *On Shoulder
    *General
    *Car Stopped
    *Animals
    *Missing Sign
  *Weather
    *General
    *Flood
    *Fog
    *Heavy Snow
    *Hail

I think that we shouldn't keep NA subtypes because we still know that it fits in that general type, but we don't have more specific information, or it doesn't fit into any of the other categories. Dashboard users may still care about outliers that don't fit into the subtype categories. 

```{python}
df['subtype']=df['subtype'].fillna('Unclassified')
```


4. 

1. 
```{python}
empty_list=[]
thirty_two=empty_list*32
hierarchical=pd.DataFrame({'type': thirty_two, 'subtype': thirty_two, 'updated_type':thirty_two, 'updated_subtype':thirty_two, 'updated_subsubtype':thirty_two
})
```

2. 

```{python}
jam=['JAM']
jam_subset=df[df['type']=='JAM']
jam_array=jam_subset['subtype'].unique()
jam_subtypes=jam_array.tolist()
jam_type=len(jam_subtypes)*jam
```

```{python}
accident=['ACCIDENT']
accident_subset=df[df['type']=='ACCIDENT']
accident_array=(accident_subset['subtype'].unique())
accident_subtypes=accident_array.tolist()
accident_type=len(accident_subtypes)*accident
```

```{python}
road_closed=['ROAD_CLOSED']
road_closed_subset=df[df['type']=='ROAD_CLOSED']
road_closed_array=road_closed_subset['subtype'].unique()
road_closed_subtypes=road_closed_array.tolist()
road_closed_type=len(road_closed_subtypes)*road_closed
```

```{python}
hazard=['HAZARD']
hazard_subset=df[df['type']=='HAZARD']
hazard_array=hazard_subset['subtype'].unique()
hazard_subtypes=hazard_array.tolist()
hazard_type=len(hazard_subtypes)*hazard
```

```{python}
all_types=jam_type+accident_type+road_closed_type+hazard_type
all_subtypes=jam_subtypes+accident_subtypes+road_closed_subtypes+hazard_subtypes
```

```{python}
#create list for updated subtype col
on_road=['ON_ROAD']
on_road_list=on_road*10
on_shoulder=['On Shoulder']
on_shoulder_list=on_shoulder*4
weather=['Weather']
weather_list=weather*5
unclassified=['Unclassified']
hazard_breakout=unclassified+on_road_list+on_shoulder_list+weather_list
new_subtypes=jam_subtypes+accident_subtypes+road_closed_subtypes+hazard_breakout
```

```{python}
#create list for updated_subsubtype col
hazard_road_subsubtypes=['General', 'Car Stopped', 'Construction', 'Emergency Vehicle', 'Ice', 'Object', 'Pot Hole', 'Traffic Light Fault', 'Lane Closed', 'Road Kill']
hazard_shoulder_subsubtypes=['General', 'Car Stopped', 'Animals', 'Missing Sign']
hazard_weather_subsubtypes=['General', 'Flood', 'Fog', 'Heavy Snow', 'Hail']
new_subsubtypes=(unclassified*13)+hazard_road_subsubtypes+hazard_shoulder_subsubtypes+hazard_weather_subsubtypes
```

```{python}
#fill in heirarchical
hierarchical['type']=all_types
hierarchical['subtype']=all_subtypes
hierarchical['updated_type']=all_types
hierarchical['updated_subtype']=new_subtypes
hierarchical['updated_subsubtype']=new_subsubtypes
```

```{python}
hierarchical.tail(10)
```

3. 

```{python}
merged=df.merge(hierarchical, on=['type','subtype'], how='left')
```

```{python}
merged.head()
```
4. 

```{python}
assert all(df['type']==merged['type']), 'Diff Type'
assert all(df['subtype']==merged['subtype']), 'Diff Subtype'
```


# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}
import re
merged.head()
```

```{python}
test='Point(-87.676685 41.929692)'

check=re.split(r'\(', test, 1)
re.split('\s', check[1], 1)
```

```{python}
def latitude(txt):
  x=re.split(r'\(', txt, 1)
  y=re.split(r'\s', x[1], 1)
  lat= y[0]
  return lat


def longitude(txt):
  x=re.split(r'\(', txt, 1)
  y=re.split(r'\s', x[1], 1)
  long=y[1]
  long=long.replace(r')', '')
  return long
```
```{python}
merged['latitude']=merged['geoWKT'].map(latitude)
merged['longitude']=merged['geoWKT'].map(longitude)
```

```{python}
merged.head()
```

b. 

```{python}
merged['latitude']=pd.to_numeric(merged['latitude'])
merged['longitude']=pd.to_numeric(merged['longitude'])
```

```{python}
merged['binned_lat'] = merged['latitude'].round(2)
merged['binned_long'] = merged['longitude'].round(2)
```

```{python}
grouped=merged.groupby(['binned_lat', 'binned_long']).size()
grouped=grouped.reset_index()
grouped.columns=['latitude', 'longitude', 'count']
```
```{python}
grouped.max()
```
(-87.56, 42.02) has the most observations with 21325. 

c. 

```{python}
subset=merged[(merged['updated_type'] == 'JAM') & (merged['updated_subtype'] == 'Unclassified')]
subset_grouped=subset.groupby(['binned_lat', 'binned_long']).size()
subset_grouped=subset_grouped.reset_index()
subset_grouped.columns=['Latitude', 'Longitude', 'Count']
subset_grouped=subset_grouped.sort_values(by='Count', ascending=False)
subset_grouped=subset_grouped.head(10)
subset_grouped['type']='JAM'
subset_grouped['subtype']='Unclassified'
print(subset_grouped)
```

```{python}
top_tens=[]
```
```{python}
for i in merged['updated_type'].unique():
  type_set=merged[merged['updated_type']==i]
  for j in type_set['updated_subtype'].unique():
    subset = merged[(merged['updated_type'] == i) & (merged['updated_subtype'] == j)]
    subset_grouped=subset.groupby(['binned_lat', 'binned_long']).size()
    subset_grouped=subset_grouped.reset_index()
    subset_grouped.columns=['Latitude', 'Longitude', 'Count']
    subset_grouped=subset_grouped.sort_values(by='Count', ascending=False)
    subset_grouped=subset_grouped.head(10)
    subset_grouped['type']=i
    subset_grouped['subtype']=j
    top_tens.append(subset_grouped)
    print(f'finished {i}:{j}, length{len(subset_grouped)}')

top_ten_df=pd.concat(top_tens, ignore_index=True)
```

```{python}
check=merged[merged['updated_type']=='JAM']
test=check[check['updated_subtype']=='JAM_LIGHT_TRAFFIC']
```

```{python}
top_ten_df.to_csv(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\top_alerts_map\top_alerts_map.csv")
```
The data is filtered based on type and subtype and then aggregated by binned latitude and binned longitude. 
The dataframe has 155 rows. It has 16 combinations of types and subtypes but type Jam with subtype Jam Light Traffic only has 5 observations so the final length isn't divisible by 10. 


2. 
```{python}
subset_jam_heavy_traffic=top_ten_df[(top_ten_df['type']=='JAM') &(top_ten_df['subtype']=='JAM_HEAVY_TRAFFIC')]
plot_heavy_traffic=alt.Chart(subset_jam_heavy_traffic).mark_point().encode(
  alt.X('Longitude:Q').scale(domain=(subset_jam_heavy_traffic['Longitude'].min(), subset_jam_heavy_traffic['Longitude'].max())),
  alt.Y('Latitude:Q').scale(domain=(subset_jam_heavy_traffic['Latitude'].min(), subset_jam_heavy_traffic['Latitude'].max()))
)
plot_heavy_traffic
```

3. 
    
a. 

```{python}

```
    

b. 
```{python}
# MODIFY ACCORDINGLY
file_path = r"C:\Users\laine\OneDrive\Documents\GitHub\student30538\problem_sets\ps6\top_alerts_map\Boundaries - Neighborhoods.geojson"
#----

with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])
```


4. 

```{python}
chi_map=alt.Chart(geo_data).mark_geoshape(
  fill='lightgray',
  stroke='white'
).properties(
  width=300,
  height=300)
chi_map+plot_heavy_traffic
```

5. 

a. 

```{python}
choices=[]
for i in merged['updated_type'].unique():
  type_set=merged[merged['updated_type']==i]
  for j in type_set['updated_subtype'].unique():
    choice=f'{i}: {j}'
    choices.append(choice)
```

```{python}
choices
```

```{python}
from PIL import Image
dropdown_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\type_subtype_dropdown_screenshot.png")
dropdown_screenshot.show()
```
There are 16 type and subtype combinations. 

```{python}
test=merged[merged['type']=='ACCIDENT']
test=test[test['subtype']=='Unclassified']
test.head()
```
b. 
```{python}
app_1_plot_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App1_plot.png")
app_1_plot_screenshot.show()
```

c. 
```{python}

```

d. 
```{python}

```

e. 

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 


    
b. 
```{python}

```

c.

```{python}

```
    

2.

a. 



b. 


c. 


# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

b. 

```{python}

```

2. 

a. 


b. 
    
3. 

a. 
    

b. 


c. 


d.
