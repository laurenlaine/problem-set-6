---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Peter Ganong, Maggie Shi, and Andre Oviedo"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*\_\_\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*\_\_\*\* (2 point)
3. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: true
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import pandas as pd
import altair as alt 
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 

import json
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
sample=pd.read_csv(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\waze_data_sample.csv")
sample.head()
```
Unnamed 0: Index
city: Nominal
confidence: Quantitative
nThumbsUp: Quantitative
street: Nominal
uuid: Nominal
country: Nominal
type: Nominal
subtype: Nominal
RoadType: Nominal
reliability: Quantitative
magvar: Nominal
reportRating: Quantitative

2. 

```{python}
df=pd.read_csv(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\waze_data.csv")
```

```{python}
def count_null(data):
  count_null=[]
  for column in data:
    num_null=len(data[data[column]=='NULL'])
    count_null.append(num_null)
  return count_null
```

```{python}
df_null_counts=count_null(df)
print(df_null_counts)
```
```{python}
def count_na(data):
  counts=[]
  for column in data:
    num_na=len(data[data[column].isna()==True])
    counts.append(num_na)
  return counts
```

```{python}
def not_na(data):
  count_not_na=[]
  for column in data:
    num_not_na=len(data[data[column].isna()==False])
    count_not_na.append(num_not_na)
  return count_not_na
```
```{python}
na_counts=count_na(df)
not_na_counts=not_na(df)
variables=df.columns
counts = pd.DataFrame({'Variable': variables, 'Na_Count': na_counts, 'Not_Na_Count': not_na_counts
})
```

```{python}
counts_melt=pd.melt(counts, id_vars=['Variable'],value_vars=['Na_Count', 'Not_Na_Count'])
counts_melt.columns=['Variable', 'Type', 'Count']
counts_melt.head()
```


```{python}
stacked_bar_na=alt.Chart(counts_melt, title='Number of Missing and Not Missing Observations by Variable').mark_bar().encode(
  alt.X('Variable:N'),
  alt.Y('Count:Q'),
  alt.Color('Type:N')
)
stacked_bar_na
```
nThumbsUp, street, and subtype all have missing values. nThumbsUp has the highest share of missing observations.


3. 

```{python}
print(df['type'].unique())
for i in (df['type'].unique()):
  subset=df[df['type']==i]
  print(f'{i}: {subset['subtype'].unique()
  }')
```
All 4 types have a nan subtype.
Hazard could have sub-subtypes- on road, on shoulder, and weather. 

*Jam
  *Na
  *Stand Still Traffic
  *Heavy Traffic
  *Moderate Traffic
  *Light Traffic
*Accident
  *Na
  *Major
  *Minor
*Road Closed
  *Na
  *Event
  *Construction
  *Hazard
*Hazard
  *Na
  *On Road
    *General
    *Car Stopped
    *Construction
    *Emergency Vehicle
    *Ice
    *Object
    *Pot Hole
    *Traffic Light Fault
    *Lane Closed
    *Road Kill
  *On Shoulder
    *General
    *Car Stopped
    *Animals
    *Missing Sign
  *Weather
    *General
    *Flood
    *Fog
    *Heavy Snow
    *Hail

I think that we shouldn't keep NA subtypes because we still know that it fits in that general type, but we don't have more specific information, or it doesn't fit into any of the other categories. Dashboard users may still care about outliers that don't fit into the subtype categories. 

```{python}
df['subtype']=df['subtype'].fillna('Unclassified')
```


4. 

1. 
```{python}
empty_list=[]
thirty_two=empty_list*32
hierarchical=pd.DataFrame({'type': thirty_two, 'subtype': thirty_two, 'updated_type':thirty_two, 'updated_subtype':thirty_two, 'updated_subsubtype':thirty_two
})
```

2. 

```{python}
jam=['JAM']
jam_subset=df[df['type']=='JAM']
jam_array=jam_subset['subtype'].unique()
jam_subtypes=jam_array.tolist()
jam_type=len(jam_subtypes)*jam
```

```{python}
accident=['ACCIDENT']
accident_subset=df[df['type']=='ACCIDENT']
accident_array=(accident_subset['subtype'].unique())
accident_subtypes=accident_array.tolist()
accident_type=len(accident_subtypes)*accident
```

```{python}
road_closed=['ROAD_CLOSED']
road_closed_subset=df[df['type']=='ROAD_CLOSED']
road_closed_array=road_closed_subset['subtype'].unique()
road_closed_subtypes=road_closed_array.tolist()
road_closed_type=len(road_closed_subtypes)*road_closed
```

```{python}
hazard=['HAZARD']
hazard_subset=df[df['type']=='HAZARD']
hazard_array=hazard_subset['subtype'].unique()
hazard_subtypes=hazard_array.tolist()
hazard_type=len(hazard_subtypes)*hazard
```

```{python}
all_types=jam_type+accident_type+road_closed_type+hazard_type
all_subtypes=jam_subtypes+accident_subtypes+road_closed_subtypes+hazard_subtypes
```

```{python}
#create list for updated subtype col
on_road=['ON_ROAD']
on_road_list=on_road*10
on_shoulder=['On Shoulder']
on_shoulder_list=on_shoulder*4
weather=['Weather']
weather_list=weather*5
unclassified=['Unclassified']
hazard_breakout=unclassified+on_road_list+on_shoulder_list+weather_list
new_subtypes=jam_subtypes+accident_subtypes+road_closed_subtypes+hazard_breakout
```

```{python}
#create list for updated_subsubtype col
hazard_road_subsubtypes=['General', 'Car Stopped', 'Construction', 'Emergency Vehicle', 'Ice', 'Object', 'Pot Hole', 'Traffic Light Fault', 'Lane Closed', 'Road Kill']
hazard_shoulder_subsubtypes=['General', 'Car Stopped', 'Animals', 'Missing Sign']
hazard_weather_subsubtypes=['General', 'Flood', 'Fog', 'Heavy Snow', 'Hail']
new_subsubtypes=(unclassified*13)+hazard_road_subsubtypes+hazard_shoulder_subsubtypes+hazard_weather_subsubtypes
```

```{python}
#fill in heirarchical
hierarchical['type']=all_types
hierarchical['subtype']=all_subtypes
hierarchical['updated_type']=all_types
hierarchical['updated_subtype']=new_subtypes
hierarchical['updated_subsubtype']=new_subsubtypes
```

```{python}
hierarchical.tail(10)
```

3. 

```{python}
merged=df.merge(hierarchical, on=['type','subtype'], how='left')
```

```{python}
merged.head()
```
4. 

```{python}
assert all(df['type']==merged['type']), 'Diff Type'
assert all(df['subtype']==merged['subtype']), 'Diff Subtype'
```


# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}
import re
merged.head()
```

```{python}
test='Point(-87.676685 41.929692)'

check=re.split(r'\(', test, 1)
re.split('\s', check[1], 1)
```

```{python}
def longitude(txt):
  x=re.split(r'\(', txt, 1)
  y=re.split(r'\s', x[1], 1)
  lat= y[0]
  return lat


def latitude(txt):
  x=re.split(r'\(', txt, 1)
  y=re.split(r'\s', x[1], 1)
  long=y[1]
  long=long.replace(r')', '')
  return long
```
```{python}
merged['latitude']=merged['geoWKT'].map(latitude)
merged['longitude']=merged['geoWKT'].map(longitude)
```

```{python}
merged.head()
```

b. 

```{python}
merged['latitude']=pd.to_numeric(merged['latitude'])
merged['longitude']=pd.to_numeric(merged['longitude'])
```

```{python}
merged['binned_lat'] = merged['latitude'].round(2)
merged['binned_long'] = merged['longitude'].round(2)
```

```{python}
grouped=merged.groupby(['binned_lat', 'binned_long']).size()
grouped=grouped.reset_index()
grouped.columns=['latitude', 'longitude', 'count']
```
```{python}
grouped.max()
```
(-87.56, 42.02) has the most observations with 21325. 

c. 

```{python}
subset=merged[(merged['updated_type'] == 'JAM') & (merged['updated_subtype'] == 'Unclassified')]
subset_grouped=subset.groupby(['binned_lat', 'binned_long']).size()
subset_grouped=subset_grouped.reset_index()
subset_grouped.columns=['Latitude', 'Longitude', 'Count']
subset_grouped=subset_grouped.sort_values(by='Count', ascending=False)
subset_grouped=subset_grouped.head(10)
subset_grouped['type']='JAM'
subset_grouped['subtype']='Unclassified'
print(subset_grouped)
```

```{python}
top_tens=[]
```
```{python}
for i in merged['updated_type'].unique():
  type_set=merged[merged['updated_type']==i]
  for j in type_set['updated_subtype'].unique():
    subset = merged[(merged['updated_type'] == i) & (merged['updated_subtype'] == j)]
    subset_grouped=subset.groupby(['binned_lat', 'binned_long']).size()
    subset_grouped=subset_grouped.reset_index()
    subset_grouped.columns=['Latitude', 'Longitude', 'Count']
    subset_grouped=subset_grouped.sort_values(by='Count', ascending=False)
    subset_grouped=subset_grouped.head(10)
    subset_grouped['type']=i
    subset_grouped['subtype']=j
    top_tens.append(subset_grouped)
    print(f'finished {i}:{j}, length{len(subset_grouped)}')

top_ten_df=pd.concat(top_tens, ignore_index=True)
```

```{python}
check=merged[merged['updated_type']=='JAM']
test=check[check['updated_subtype']=='JAM_LIGHT_TRAFFIC']
```

```{python}
top_ten_df.to_csv(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\top_alerts_map\top_alerts_map.csv")
```
The data is filtered based on type and subtype and then aggregated by binned latitude and binned longitude. 
The dataframe has 155 rows. It has 16 combinations of types and subtypes but type Jam with subtype Jam Light Traffic only has 5 observations so the final length isn't divisible by 10. 


2. 

```{python}
max_long=top_ten_df['Longitude'].max()
min_long=top_ten_df['Longitude'].min()
print(min_long, max_long)
max_lat=top_ten_df['Latitude'].max()
min_lat=top_ten_df['Latitude'].min()
print(min_lat, max_lat)
```
```{python}
subset_jam_heavy_traffic=top_ten_df[(top_ten_df['type']=='JAM') &(top_ten_df['subtype']=='JAM_HEAVY_TRAFFIC')]
plot_heavy_traffic=alt.Chart(subset_jam_heavy_traffic).mark_point().encode(
  alt.X('Longitude:Q').scale(domain=(min_long, max_long)),
  alt.Y('Latitude:Q').scale(domain=(min_lat, max_lat)),
  alt.Size('Count:Q')
)
plot_heavy_traffic
```

3. 
    
a. 

```{python}

```
    

b. 
```{python}
# MODIFY ACCORDINGLY
file_path = r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Boundaries - Neighborhoods .geojson"
#----

with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])
```


4. 

```{python}
chi_map=alt.Chart(geo_data).mark_geoshape(
  fill='lightgray',
  stroke='white'
).properties(
  width=375,
  height=450
)
chi_map+plot_heavy_traffic
```

5. 

a. 

```{python}
choices=[]
for i in merged['updated_type'].unique():
  type_set=merged[merged['updated_type']==i]
  for j in type_set['updated_subtype'].unique():
    choice=f'{i}: {j}'
    choices.append(choice)
```

```{python}
#print choices to put in choices list in the app
choices
```

```{python}
from PIL import Image
dropdown_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\type_subtype_dropdown_screenshot.png")
dropdown_screenshot.show()
```
There are 16 type and subtype combinations. 

```{python}
test=merged[merged['type']=='ACCIDENT']
test=test[test['subtype']=='Unclassified']
test.head()
```
b. 
```{python}
app_1_plot_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App1_plot.png")
app_1_plot_screenshot.show()
```

c. 
```{python}
app_1_road_closed_events_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App1_road_closed_events.png")
app_1_plot_screenshot.show()
```


d. 
Where do the most major accidents occur? 
```{python}
app_1_major_accidents_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App1_major_accidents.png")
app_1_major_accidents_screenshot.show()

```

e. 
It may also be interesting to look at roadType to see if there are any common road types that involved in accidents, or other alerts. That could be incorporated into the app using color as roadtype. 

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. I don't think it would be a good idea to collapse the dataset by this column for two reasons. First, time is very specific if grouped each different time would be seperate, ex not grouped by hour or number of hours. This wouldn't be very helpful in making trends readily apparent without having to inspect each specific time. Also the column has times in UTC not in Chicago's central time so the time could actually be misleading. 


    
b. 
```{python}
merged['ts']=pd.to_datetime(merged['ts'])
merged['hour']=merged['ts'].map(lambda x: x.time().hour)

```

```{python}
top_tens_by_hour=[]
```
```{python}

for x in merged['hour'].unique():
  hour_set=merged[merged['hour']==x]
  for i in hour_set['updated_type'].unique():
    type_set=hour_set[hour_set['updated_type']==i]
    for j in type_set['updated_subtype'].unique():
      subset = type_set[type_set['updated_subtype']==j]
      subset_grouped=subset.groupby(['binned_lat', 'binned_long']).size()
      subset_grouped=subset_grouped.reset_index()
      subset_grouped.columns=['Latitude', 'Longitude', 'Count']
      subset_grouped=subset_grouped.sort_values(by='Count', ascending=False)
      subset_grouped=subset_grouped.head(10)
      subset_grouped['hour']=x
      subset_grouped['type']=i
      subset_grouped['subtype']=j
      top_tens_by_hour.append(subset_grouped)
      print(f'finished {x}:{i}:{j}, length{len(subset_grouped)}')

top_ten_by_hourdf=pd.concat(top_tens_by_hour, ignore_index=True)
```

```{python}
top_ten_by_hourdf.to_csv(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\top_alerts_map_byhour\top_alerts_map_byhour.csv")
```

c.

```{python}
heavy_traffic_at_1=top_ten_by_hourdf[top_ten_by_hourdf['hour']==1]
heavy_traffic_at_1=heavy_traffic_at_1[heavy_traffic_at_1['type']=='JAM']
heavy_traffic_at_1=heavy_traffic_at_1[heavy_traffic_at_1['subtype']=='JAM_HEAVY_TRAFFIC']
```
  

```{python}
traffic_at_1_plot=alt.Chart(heavy_traffic_at_1).mark_point().encode(
  alt.X('Longitude:Q').scale(domain=(min_long, max_long)),
  alt.Y('Latitude:Q').scale(domain=(min_lat, max_lat)),
  alt.Size('Count:Q'))

chi_map+traffic_at_1_plot
```

```{python}
heavy_traffic_at_6=top_ten_by_hourdf[top_ten_by_hourdf['hour']==6]
heavy_traffic_at_6=heavy_traffic_at_6[heavy_traffic_at_6['type']=='JAM']
heavy_traffic_at_6=heavy_traffic_at_6[heavy_traffic_at_6['subtype']=='JAM_HEAVY_TRAFFIC']
```
  

```{python}
traffic_at_6_plot=alt.Chart(heavy_traffic_at_6).mark_point().encode(
  alt.X('Longitude:Q').scale(domain=(min_long, max_long)),
  alt.Y('Latitude:Q').scale(domain=(min_lat, max_lat)),
  alt.Size('Count:Q'))

chi_map+traffic_at_6_plot
```

```{python}
heavy_traffic_at_12=top_ten_by_hourdf[top_ten_by_hourdf['hour']==12]
heavy_traffic_at_12=heavy_traffic_at_12[heavy_traffic_at_12['type']=='JAM']
heavy_traffic_at_12=heavy_traffic_at_12[heavy_traffic_at_12['subtype']=='JAM_HEAVY_TRAFFIC']
```
  

```{python}
traffic_at_12_plot=alt.Chart(heavy_traffic_at_12).mark_point().encode(
  alt.X('Longitude:Q').scale(domain=(min_long, max_long)),
  alt.Y('Latitude:Q').scale(domain=(min_lat, max_lat)),
  alt.Size('Count:Q'))

chi_map+traffic_at_12_plot
```


2.

a. 

```{python}
app2_ui=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App2_slider_dropdown.png")
app2_ui.show()
```

b. 
```{python}
traffic_at_1_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App2_at_1.png")
traffic_at_1_screenshot.show()
```

```{python}
traffic_at_6_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App2_at_6.png")
traffic_at_6_screenshot.show()
```

```{python}
traffic_at_12_screenshot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App2_at_12.png")
traffic_at_12_screenshot.show()
```

c. 
From the dashboard it appears that construction is done more at night than in the morning.

3:00am UTC is equivalent to 9:00am CST in Chicago. 

```{python}
construction_at_3=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\construction_at_3.png")
construction_at_3.show()
```

15:00 UTC is equivalent to 3:00pm or 9:00pm CST in Chicago.
```{python}
construction_at_15=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\construction_at_15.png")
construction_at_15.show()
```

# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a.
It would be impractical to collapese the data by alert type and range or hours because there are so many potential ranges of hours. The resulting dataframe would be extremely long. 
It would be much more efficient to filter the data using >= and <= operators with the existing byhour dataframe than to run the top ten alerts by type for every potential range of hours. 

b. 

```{python}
max_long=top_ten_by_hourdf['Longitude'].max()
min_long=top_ten_by_hourdf['Longitude'].min()
print(min_long, max_long)
max_lat=top_ten_by_hourdf['Latitude'].max()
min_lat=top_ten_by_hourdf['Latitude'].min()
print(min_lat, max_lat)

```
```{python}
heavy_traffic_6_to_9=top_ten_by_hourdf[(top_ten_by_hourdf['hour']>=6)& (top_ten_by_hourdf['hour']<=9)]
heavy_traffic_6_to_9=heavy_traffic_6_to_9[(heavy_traffic_6_to_9['type']=='JAM')& (heavy_traffic_6_to_9['subtype']=='JAM_HEAVY_TRAFFIC')]

heavy_traffic_6_to_9=alt.Chart(heavy_traffic_6_to_9).mark_point().encode(
  alt.X('Longitude:Q').scale(domain=(min_long, max_long)),
  alt.Y('Latitude:Q').scale(domain=(min_lat, max_lat)),
  alt.Size('Count:Q'))

chi_map+heavy_traffic_6_to_9

```

```{python}
merged.head
```
2. 

a. 

```{python}
app3_ui=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App3_ui.png")
app3_ui.show()
```

b. 

```{python}
app3_heavy_traffic=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App3_heavy_traffic_6_to_9.png")
app3_heavy_traffic.show
```
    
3. 

a. 

```{python}
app3_switch_button=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App3_switch_button.png")\
app3_switch_button.show()
```
The possible values for input.switch_button are True meaning the button would start switched to hour range and False meaning the button would start switched to just hour.    

b. 

```{python}
app3_one_hour=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App3_one_hour.png")
app3_one_hour.show()
```

```{python}
app3_hour_range=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App3_range_of_hours.png")
app3_hour_range.show()
```

c. 

```{python}
app3_one_hour_plot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App3_plot_one_hour.png")
app3_one_hour_plot.show()
```

```{python}
app3_hour_range_plot=Image.open(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-6\Screenshots\App3_plot_hour_range.png")
app3_hour_range_plot.show()
```


d.
